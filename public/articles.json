{
    "status": "ok",
    "feed": {
        "url": "https://medium.com/feed/@rajatasusual",
        "title": "Stories by Rajat on Medium",
        "link": "https://medium.com/@rajatasusual?source=rss-07ca2cd9b1d2------2",
        "author": "",
        "description": "Stories by Rajat on Medium",
        "image": "https://cdn-images-1.medium.com/fit/c/150/150/1*xmvbdiLQ-1r-SK_C7_TVHg.jpeg"
    },
    "items": [
        {
            "title": "Gemini-MUD: A Technical Exploration of AI-Powered Procedural Generation",
            "pubDate": "2024-08-14 17:30:01",
            "link": "https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/6b52186361d0",
            "author": "Rajat",
            "thumbnail": "",
            "description": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*U2TwLHzU2C4VwVeC8RvdiQ.png\" width=\"2940\"></a></p>\n<p class=\"medium-feed-snippet\">In our quest to unravel the mysteries of AIâ€™s capabilities, weâ€™ve previously witnessed language models engage in a playful battle of witsâ€¦</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa Â»</a></p>\n</div>",
            "content": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*U2TwLHzU2C4VwVeC8RvdiQ.png\" width=\"2940\"></a></p>\n<p class=\"medium-feed-snippet\">In our quest to unravel the mysteries of AIâ€™s capabilities, weâ€™ve previously witnessed language models engage in a playful battle of witsâ€¦</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/gemini-mud-a-technical-exploration-of-ai-powered-procedural-generation-6b52186361d0?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa Â»</a></p>\n</div>",
            "enclosure": {},
            "categories": [
                "generative-ai-tools",
                "game-development",
                "generative-art",
                "artificial-intelligence",
                "llm"
            ]
        },
        {
            "title": "When AIâ€™s Generative Spirit Meets the Rigour of System Design",
            "pubDate": "2024-08-14 17:21:53",
            "link": "https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/85eb0ee218f2",
            "author": "Rajat",
            "thumbnail": "",
            "description": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*WUUdoGN6Fpy8kocz\" width=\"7952\"></a></p>\n<p class=\"medium-feed-snippet\">In the spirit of turning the tables on AI, much like my previous exploration into the playful â€œTur(n)ing the Tablesâ€ experiment, I nowâ€¦</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa Â»</a></p>\n</div>",
            "content": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/0*WUUdoGN6Fpy8kocz\" width=\"7952\"></a></p>\n<p class=\"medium-feed-snippet\">In the spirit of turning the tables on AI, much like my previous exploration into the playful â€œTur(n)ing the Tablesâ€ experiment, I nowâ€¦</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/when-ais-generative-spirit-meets-the-rigour-of-system-design-85eb0ee218f2?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa Â»</a></p>\n</div>",
            "enclosure": {},
            "categories": [
                "generative-ai-use-cases",
                "procedural-generation",
                "artificial-intelligence",
                "llm",
                "gaming"
            ]
        },
        {
            "title": "Tur(n)ing the Tablesâ€Šâ€”â€ŠMaking Language Models Fight.",
            "pubDate": "2024-07-30 08:02:59",
            "link": "https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/cfa0bc168878",
            "author": "Rajat",
            "thumbnail": "",
            "description": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*fOe2oo0kSm3pFJsGAttYAw.png\" width=\"2608\"></a></p>\n<p class=\"medium-feed-snippet\">A fun take on a famous experiment, the Turing Test, and with a similar objectiveâ€Šâ€”â€Šhow test how smart (or deceptive) a machine is. Theâ€¦</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa Â»</a></p>\n</div>",
            "content": "<div class=\"medium-feed-item\">\n<p class=\"medium-feed-image\"><a href=\"https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2\"><img src=\"https://cdn-images-1.medium.com/max/2600/1*fOe2oo0kSm3pFJsGAttYAw.png\" width=\"2608\"></a></p>\n<p class=\"medium-feed-snippet\">A fun take on a famous experiment, the Turing Test, and with a similar objectiveâ€Šâ€”â€Šhow test how smart (or deceptive) a machine is. Theâ€¦</p>\n<p class=\"medium-feed-link\"><a href=\"https://medium.com/taming-the-llama/turing-the-tables-making-language-models-fight-cfa0bc168878?source=rss-07ca2cd9b1d2------2\">Continue reading on Taming the LLaMa Â»</a></p>\n</div>",
            "enclosure": {},
            "categories": [
                "game-development",
                "language-model",
                "technology",
                "data-science",
                "artificial-intelligence"
            ]
        },
        {
            "title": "Bedrock of Language Modelsâ€Šâ€”â€ŠVector Embeddings, Visualised.",
            "pubDate": "2024-07-20 19:50:13",
            "link": "https://medium.com/taming-the-llama/bedrock-of-language-models-vector-embeddings-visualised-a83fcb6209af?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/a83fcb6209af",
            "author": "Rajat",
            "thumbnail": "",
            "description": "\n<h3>Bedrock of Language Modelsâ€Šâ€”â€ŠVector Embeddings, Visualised.</h3>\n<figure><img alt=\"Vector Embeddings Smoothened using Moving averages plotted on 2D space\" src=\"https://cdn-images-1.medium.com/max/800/1*8m_5HMn0THG402smMTCJuA.png\"><figcaption><a href=\"https://github.com/rajatasusual/realtime-vector-embeddings.git\">https://github.com/rajatasusual/realtime-vector-embeddings</a></figcaption></figure><blockquote>The article demonstrates how to understand the way NLP systems breakdown content into vectors into multi-dimensional space and then connect the dots. It is incredibly complex for a human brain to visualise higher dimensional spaces but for machines it is easy business. So, I created a simple yet what I believe is a novel solution to help us comprehend thisÂ concept.</blockquote>\n<blockquote>check out: <a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a>\n</blockquote>\n<blockquote>Shout out to <a href=\"https://medium.com/u/94d7e8f6238c\">Sam Gallagher</a> and his incredible article (<a href=\"https://medium.com/@gallaghersam95/visualizing-embedding-vectors-99cac1d164c4\">Visualizing embedding vectors</a>) for the inspiration.</blockquote>\n<h3>Introduction</h3>\n<p>Vectors are fundamental to modern Natural Language Processing (NLP) and by extension to Large Language Models (LLMs). They are a means to represent text (or any context) in a numerical format that machines can process. The definition of the vector remains the same that we remember from high schoolÂ â€”</p>\n<blockquote>A geometric object that has a length and magnitude in n dimensional space.</blockquote>\n<p>Now, An <strong>embedding model </strong>is a special algorithm that is designed to put each word, each sentence into these dimensionsâ€” for simpler terms, these dimensions can be thought of as attibutes. Even the simplest of these embedding models classify each word into ~1000 dimensions.</p>\n<p><em>How does that look mathematically?</em> Well, itâ€™s simple. It is just a an array of say 1000 numbers where each number ranges from -x to x where x represents related to that particular dimension or attribute. Thatâ€™s how a machine understands similarity and draws semantic meaning from theÂ input.</p>\n<p>By capturing semantic meanings in a high-dimensional space, vectors allow models to understand and manipulate language in sophisticated ways. But you know it is just mathematics at the end of itâ€Šâ€”â€Šgeometry and statistics.</p>\n<h4>Basic groundwork to understand vector embeddings further</h4>\n<p>Consider two sentences:<br>1. â€œThe cat sits on the mat.â€<br>2. â€œA dog lies on theÂ rug.â€</p>\n<p>An embedding model doesnâ€™t care what a dog or a cat is but because it is trained on such a huge corpus of data, it knows cat or dog belong in somewhat similar category (without knowing the concept of what â€œanimalsâ€ are). So it converts the above sentences into numerical arrays, vectors. For simplicity, letâ€™s assume the vectors for these sentences are:<br>- Sentence 1: `[0.1, 0.2, 0.3, 0.4, 0.5]`<br>- Sentence 2: `[0.2, 0.1, 0.4, 0.3,Â 0.5]`</p>\n<p>These vectors encapsulate the semantic meaning of the sentences in a multi-dimensional space, in this case a 5 dimension space. The closer these vectors are, the more similar the sentences are considered toÂ be.</p>\n<p>Consider another sentence:</p>\n<p>3. â€œSun is shining, flowers are bloomingâ€.</p>\n<p>It might be converted to a vector like [-0.7, 2.2, 1.3, -2.7,Â 4.5].</p>\n<p>Another oneÂ like:</p>\n<p>4. â€œHorse isÂ hugeâ€</p>\n<p>This would be slightly more similar to the first two sentences, something like: [0.1, 1.2, -0.7, 1.7,Â 0.5].</p>\n<p>Now, in this example that doesnâ€™t require a graphics processor to run, you might notice that a few of the dimensions in sentences 1,2 &amp; 4 are very, very close. Thatâ€™s it. Thatâ€™s how a machine would figure what you are talking about, again without knowing what an animal is, what a rug is or what sunÂ is.</p>\n<p>We will touch how a Language model finds similarity between two texts a bitÂ later.</p>\n<h3>The Visualization Challenge</h3>\n<h4>Why does this project evenÂ exist?</h4>\n<p>Iâ€™ve been diving into projects where embedding vectors are crucial. These vectors live in dimensions beyond our visual grasp, so we use cosine similarity to make sense of them. But, I wanted moreâ€Šâ€”â€ŠI wanted to seeÂ them.</p>\n<h4>A bitÂ moreâ€¦</h4>\n<p>As I have been building a comprehensive Retrieval-Augmented Generation (RAG) system, I want to visualize the output I have been receiving by machine into something my low on GPU power brain canÂ process.</p>\n<p>While itâ€™s cool that math and tech can tell us this, I wanted to actually see these vectors. Itâ€™s one thing for math to say theyâ€™re similar; itâ€™s another to see it with our ownÂ eyes.</p>\n<h3>Letâ€™s Get intoÂ it</h3>\n<p>The premise is simple. We use an embedding model to convert a text input and map it on a 2 dimensional axis.</p>\n<figure><img alt=\"Vector embedding plotted on 2D with x axis as dimension and y, the magnitude.\" src=\"https://cdn-images-1.medium.com/max/800/1*u4DFZoXYHXSSuTAgxsdAPA.png\"><figcaption>vector embedding generated by nomic-embed-text model for word â€œtestâ€ mapped in 2DÂ space.</figcaption></figure><p>Not very clean, is it? Brace yourself when I tell you that the embedding model spit this out only for the input of â€œTestâ€. 700+ dimensions!</p>\n<p>Draw an analogy to a 3 dimensional vector asÂ follows:</p>\n<figure><img alt=\"source: https://www.intmath.com/vectors/7-vectors-in-3d-space.php\" src=\"https://cdn-images-1.medium.com/max/313/1*hKyMhar0H0DzyQBoNwh2rA.png\"><figcaption>A simple 3 dimensional vector</figcaption></figure><p>Now letâ€™s see how two text inputs which are converted to vectors lookÂ like:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*3QTB5_gWRVWHcOi0rSPBlg.png\"><figcaption>vector embeddings generated by nomic-embed-text model for words â€œtestâ€ and â€œexamâ€ mapped in 2DÂ space.</figcaption></figure><p>Still doesnâ€™t make sense on what we are looking at or if the inputs are similar or not, does it? Letâ€™s see how a machine compares twoÂ inputs.</p>\n<h4>Cosine Similarity</h4>\n<p>Cosine similarity is by far the most common way a machine identifies whether two inputs are related. It is a metric used to measure how similar two vectors are, based on the cosine of the angle between them. It ranges from -1 (completely dissimilar) to 1 (completely similar), surprisingly the same range of any cosine angle. The math is simple. here itÂ is.</p>\n<h4>Calculation</h4>\n<p>Hereâ€™s the code that forms the bedrock LLMs retrieval (itâ€™s simplified in case you were wondering):</p>\n<pre>function calculateCosineSimilarity(vec1, vec2) {<br> const dotProduct = vec1.reduce((acc, val, idx) =&gt; acc + val * vec2[idx], 0);<br> const magnitude1 = Math.sqrt(vec1.reduce((acc, val) =&gt; acc + val * val, 0));<br> const magnitude2 = Math.sqrt(vec2.reduce((acc, val) =&gt; acc + val * val, 0));<br> return dotProduct / (magnitude1 * magnitude2);<br>}<br><br>// Example vectors<br>const vectorA = [1, 2, 3];<br>const vectorB = [4, 5, 6];<br>const similarity = calculateCosineSimilarity(vectorA, vectorB);<br>console.log(`Cosine Similarity: ${similarity.toFixed(2)}`); // Output: Cosine Similarity: 0.97</pre>\n<p>But we are still stuck in understanding how to figure out visually if two text inputs are similar. So, we will simplify the machine input and hereÂ comes:</p>\n<h4>The Need for Smoothing</h4>\n<p>When visualizing vector embeddings, especially with large datasets, the graph can become cluttered and noisy, as we sawÂ above.</p>\n<blockquote>Smoothing helps by averaging values over a specified window, reducing noise and making the graph more interpretable.</blockquote>\n<h4>Why Smoothing?</h4>\n<p>Smoothing is crucial for:<br>1. <strong>Clarity</strong>: Reduces the visual noise, making patterns more evident.<br>2. <strong>Interpretation</strong>: Helps in identifying trends and relationships that may be obscured by raw dataÂ points.</p>\n<h4>Implementing Smoothing</h4>\n<p>The smoothing function helps in reducing the noise in the data by averaging the values over a specified window size. This makes the graph more readable and allows for better visualization of the overall trends in theÂ data.</p>\n<pre>const smoothData = (data, windowSize) =&gt; {<br>    const smoothed = [];<br>    for (let i = 0; i &lt; data.length; i++) {<br>        const start = Math.max(0, i - Math.floor(windowSize / 2));<br>        const end = Math.min(data.length, i + Math.floor(windowSize / 2) + 1);<br>        const window = data.slice(start, end);<br>        const average = window.reduce((sum, val) =&gt; sum + val, 0) / window.length;<br>        smoothed.push(average);<br>    }<br>    return smoothed;<br>};</pre>\n<p>And then when you visualize the graph, it looks something likeÂ this:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*gsh8zoGGk2CaM_OmoCtpUQ.png\"></figure><p>Easier to see which inputs are similar, correct? I rest myÂ case.</p>\n<h3>Application Setup</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*plRshCNiLEkAfwgpqT12jw.png\"><figcaption><a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a></figcaption></figure><p>The entire application is open sourced <a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a></p>\n<p>You will find that it has CLI and UI modes and straightforward instructions to set up. It runs completely locally.</p>\n<p>Are you curious to explore more about how to learn more about LLMs being run locally with your own data, securely? Read this: <a href=\"https://medium.com/@rajatasusual/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4\">https://medium.com/@rajatasusual/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4</a></p>\n<blockquote>Follow: <a href=\"https://github.com/rajatasusual/llamapp\">https://github.com/rajatasusual/llamapp</a> to knowÂ more.</blockquote>\n<h3>Final Thoughts</h3>\n<p>Vectors play a pivotal role in NLP and LLMs by providing a numerical representation of text. Visualizing these vectors is challenging but crucial for understanding the relationships between different pieces of text. I have tried to simplify this by providing an interactive tool to visualize and comprehend vector embeddings. Try it out and see how text relates in multi-dimensional space!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a83fcb6209af\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/bedrock-of-language-models-vector-embeddings-visualised-a83fcb6209af\">Bedrock of Language Modelsâ€Šâ€”â€ŠVector Embeddings, Visualised.</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "content": "\n<h3>Bedrock of Language Modelsâ€Šâ€”â€ŠVector Embeddings, Visualised.</h3>\n<figure><img alt=\"Vector Embeddings Smoothened using Moving averages plotted on 2D space\" src=\"https://cdn-images-1.medium.com/max/800/1*8m_5HMn0THG402smMTCJuA.png\"><figcaption><a href=\"https://github.com/rajatasusual/realtime-vector-embeddings.git\">https://github.com/rajatasusual/realtime-vector-embeddings</a></figcaption></figure><blockquote>The article demonstrates how to understand the way NLP systems breakdown content into vectors into multi-dimensional space and then connect the dots. It is incredibly complex for a human brain to visualise higher dimensional spaces but for machines it is easy business. So, I created a simple yet what I believe is a novel solution to help us comprehend thisÂ concept.</blockquote>\n<blockquote>check out: <a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a>\n</blockquote>\n<blockquote>Shout out to <a href=\"https://medium.com/u/94d7e8f6238c\">Sam Gallagher</a> and his incredible article (<a href=\"https://medium.com/@gallaghersam95/visualizing-embedding-vectors-99cac1d164c4\">Visualizing embedding vectors</a>) for the inspiration.</blockquote>\n<h3>Introduction</h3>\n<p>Vectors are fundamental to modern Natural Language Processing (NLP) and by extension to Large Language Models (LLMs). They are a means to represent text (or any context) in a numerical format that machines can process. The definition of the vector remains the same that we remember from high schoolÂ â€”</p>\n<blockquote>A geometric object that has a length and magnitude in n dimensional space.</blockquote>\n<p>Now, An <strong>embedding model </strong>is a special algorithm that is designed to put each word, each sentence into these dimensionsâ€” for simpler terms, these dimensions can be thought of as attibutes. Even the simplest of these embedding models classify each word into ~1000 dimensions.</p>\n<p><em>How does that look mathematically?</em> Well, itâ€™s simple. It is just a an array of say 1000 numbers where each number ranges from -x to x where x represents related to that particular dimension or attribute. Thatâ€™s how a machine understands similarity and draws semantic meaning from theÂ input.</p>\n<p>By capturing semantic meanings in a high-dimensional space, vectors allow models to understand and manipulate language in sophisticated ways. But you know it is just mathematics at the end of itâ€Šâ€”â€Šgeometry and statistics.</p>\n<h4>Basic groundwork to understand vector embeddings further</h4>\n<p>Consider two sentences:<br>1. â€œThe cat sits on the mat.â€<br>2. â€œA dog lies on theÂ rug.â€</p>\n<p>An embedding model doesnâ€™t care what a dog or a cat is but because it is trained on such a huge corpus of data, it knows cat or dog belong in somewhat similar category (without knowing the concept of what â€œanimalsâ€ are). So it converts the above sentences into numerical arrays, vectors. For simplicity, letâ€™s assume the vectors for these sentences are:<br>- Sentence 1: `[0.1, 0.2, 0.3, 0.4, 0.5]`<br>- Sentence 2: `[0.2, 0.1, 0.4, 0.3,Â 0.5]`</p>\n<p>These vectors encapsulate the semantic meaning of the sentences in a multi-dimensional space, in this case a 5 dimension space. The closer these vectors are, the more similar the sentences are considered toÂ be.</p>\n<p>Consider another sentence:</p>\n<p>3. â€œSun is shining, flowers are bloomingâ€.</p>\n<p>It might be converted to a vector like [-0.7, 2.2, 1.3, -2.7,Â 4.5].</p>\n<p>Another oneÂ like:</p>\n<p>4. â€œHorse isÂ hugeâ€</p>\n<p>This would be slightly more similar to the first two sentences, something like: [0.1, 1.2, -0.7, 1.7,Â 0.5].</p>\n<p>Now, in this example that doesnâ€™t require a graphics processor to run, you might notice that a few of the dimensions in sentences 1,2 &amp; 4 are very, very close. Thatâ€™s it. Thatâ€™s how a machine would figure what you are talking about, again without knowing what an animal is, what a rug is or what sunÂ is.</p>\n<p>We will touch how a Language model finds similarity between two texts a bitÂ later.</p>\n<h3>The Visualization Challenge</h3>\n<h4>Why does this project evenÂ exist?</h4>\n<p>Iâ€™ve been diving into projects where embedding vectors are crucial. These vectors live in dimensions beyond our visual grasp, so we use cosine similarity to make sense of them. But, I wanted moreâ€Šâ€”â€ŠI wanted to seeÂ them.</p>\n<h4>A bitÂ moreâ€¦</h4>\n<p>As I have been building a comprehensive Retrieval-Augmented Generation (RAG) system, I want to visualize the output I have been receiving by machine into something my low on GPU power brain canÂ process.</p>\n<p>While itâ€™s cool that math and tech can tell us this, I wanted to actually see these vectors. Itâ€™s one thing for math to say theyâ€™re similar; itâ€™s another to see it with our ownÂ eyes.</p>\n<h3>Letâ€™s Get intoÂ it</h3>\n<p>The premise is simple. We use an embedding model to convert a text input and map it on a 2 dimensional axis.</p>\n<figure><img alt=\"Vector embedding plotted on 2D with x axis as dimension and y, the magnitude.\" src=\"https://cdn-images-1.medium.com/max/800/1*u4DFZoXYHXSSuTAgxsdAPA.png\"><figcaption>vector embedding generated by nomic-embed-text model for word â€œtestâ€ mapped in 2DÂ space.</figcaption></figure><p>Not very clean, is it? Brace yourself when I tell you that the embedding model spit this out only for the input of â€œTestâ€. 700+ dimensions!</p>\n<p>Draw an analogy to a 3 dimensional vector asÂ follows:</p>\n<figure><img alt=\"source: https://www.intmath.com/vectors/7-vectors-in-3d-space.php\" src=\"https://cdn-images-1.medium.com/max/313/1*hKyMhar0H0DzyQBoNwh2rA.png\"><figcaption>A simple 3 dimensional vector</figcaption></figure><p>Now letâ€™s see how two text inputs which are converted to vectors lookÂ like:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*3QTB5_gWRVWHcOi0rSPBlg.png\"><figcaption>vector embeddings generated by nomic-embed-text model for words â€œtestâ€ and â€œexamâ€ mapped in 2DÂ space.</figcaption></figure><p>Still doesnâ€™t make sense on what we are looking at or if the inputs are similar or not, does it? Letâ€™s see how a machine compares twoÂ inputs.</p>\n<h4>Cosine Similarity</h4>\n<p>Cosine similarity is by far the most common way a machine identifies whether two inputs are related. It is a metric used to measure how similar two vectors are, based on the cosine of the angle between them. It ranges from -1 (completely dissimilar) to 1 (completely similar), surprisingly the same range of any cosine angle. The math is simple. here itÂ is.</p>\n<h4>Calculation</h4>\n<p>Hereâ€™s the code that forms the bedrock LLMs retrieval (itâ€™s simplified in case you were wondering):</p>\n<pre>function calculateCosineSimilarity(vec1, vec2) {<br> const dotProduct = vec1.reduce((acc, val, idx) =&gt; acc + val * vec2[idx], 0);<br> const magnitude1 = Math.sqrt(vec1.reduce((acc, val) =&gt; acc + val * val, 0));<br> const magnitude2 = Math.sqrt(vec2.reduce((acc, val) =&gt; acc + val * val, 0));<br> return dotProduct / (magnitude1 * magnitude2);<br>}<br><br>// Example vectors<br>const vectorA = [1, 2, 3];<br>const vectorB = [4, 5, 6];<br>const similarity = calculateCosineSimilarity(vectorA, vectorB);<br>console.log(`Cosine Similarity: ${similarity.toFixed(2)}`); // Output: Cosine Similarity: 0.97</pre>\n<p>But we are still stuck in understanding how to figure out visually if two text inputs are similar. So, we will simplify the machine input and hereÂ comes:</p>\n<h4>The Need for Smoothing</h4>\n<p>When visualizing vector embeddings, especially with large datasets, the graph can become cluttered and noisy, as we sawÂ above.</p>\n<blockquote>Smoothing helps by averaging values over a specified window, reducing noise and making the graph more interpretable.</blockquote>\n<h4>Why Smoothing?</h4>\n<p>Smoothing is crucial for:<br>1. <strong>Clarity</strong>: Reduces the visual noise, making patterns more evident.<br>2. <strong>Interpretation</strong>: Helps in identifying trends and relationships that may be obscured by raw dataÂ points.</p>\n<h4>Implementing Smoothing</h4>\n<p>The smoothing function helps in reducing the noise in the data by averaging the values over a specified window size. This makes the graph more readable and allows for better visualization of the overall trends in theÂ data.</p>\n<pre>const smoothData = (data, windowSize) =&gt; {<br>    const smoothed = [];<br>    for (let i = 0; i &lt; data.length; i++) {<br>        const start = Math.max(0, i - Math.floor(windowSize / 2));<br>        const end = Math.min(data.length, i + Math.floor(windowSize / 2) + 1);<br>        const window = data.slice(start, end);<br>        const average = window.reduce((sum, val) =&gt; sum + val, 0) / window.length;<br>        smoothed.push(average);<br>    }<br>    return smoothed;<br>};</pre>\n<p>And then when you visualize the graph, it looks something likeÂ this:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/1*gsh8zoGGk2CaM_OmoCtpUQ.png\"></figure><p>Easier to see which inputs are similar, correct? I rest myÂ case.</p>\n<h3>Application Setup</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*plRshCNiLEkAfwgpqT12jw.png\"><figcaption><a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a></figcaption></figure><p>The entire application is open sourced <a href=\"https://github.com/rajatasusual/realtime-vector-embeddings\">https://github.com/rajatasusual/realtime-vector-embeddings</a></p>\n<p>You will find that it has CLI and UI modes and straightforward instructions to set up. It runs completely locally.</p>\n<p>Are you curious to explore more about how to learn more about LLMs being run locally with your own data, securely? Read this: <a href=\"https://medium.com/@rajatasusual/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4\">https://medium.com/@rajatasusual/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4</a></p>\n<blockquote>Follow: <a href=\"https://github.com/rajatasusual/llamapp\">https://github.com/rajatasusual/llamapp</a> to knowÂ more.</blockquote>\n<h3>Final Thoughts</h3>\n<p>Vectors play a pivotal role in NLP and LLMs by providing a numerical representation of text. Visualizing these vectors is challenging but crucial for understanding the relationships between different pieces of text. I have tried to simplify this by providing an interactive tool to visualize and comprehend vector embeddings. Try it out and see how text relates in multi-dimensional space!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a83fcb6209af\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/bedrock-of-language-models-vector-embeddings-visualised-a83fcb6209af\">Bedrock of Language Modelsâ€Šâ€”â€ŠVector Embeddings, Visualised.</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "enclosure": {},
            "categories": [
                "nlp",
                "visualization",
                "llm",
                "artificial-intelligence",
                "data-science"
            ]
        },
        {
            "title": "Tame Artificial Intelligence, from your laptop",
            "pubDate": "2024-07-18 17:41:59",
            "link": "https://medium.com/taming-the-llama/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/d91d3d38b9b4",
            "author": "Rajat",
            "thumbnail": "",
            "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PbGZF4nbTbblAmAijk7Kdw.png\"><figcaption>llamapp beta. (<a href=\"http://github.com/rajatasusual/llamapp\">http://github.com/rajatasusual/llamapp</a>)</figcaption></figure><blockquote>This is a personal project (<a href=\"http://github.com/rajatasusual/llamapp\">http://github.com/rajatasusual/llamapp</a> ) where I explore the extents of layer 2 solutions on top of AI models all run locallyâ€Šâ€”â€Šcontextually aware, no hallucinations and secure.(Shoutout to <strong>Langchain</strong>(<a href=\"https://github.com/langchain-ai\">https://github.com/langchain-ai</a>), a really cool company enabling the â€œpowered by AIâ€ era. P.S. the actual powered by AI and not just a prompt hiding behind a beautiful UI)</blockquote>\n<p>You know the feeling when you get a new laptop and you immediately want to see what breaks it? No? Maybe itâ€™sÂ me.</p>\n<p>Well turns out, running a super powerful (yet tiny by AI standards) <a href=\"https://ai.google.dev/gemma?trk=article-ssr-frontend-pulse_little-text-block\">Google Gemma</a>Â , a vector DB (<a href=\"http://redis.io/\">http://redis.io</a>) and a local server doesnâ€™t. And that it gives far better results than expected.</p>\n<h3>The ElevatorÂ Pitch</h3>\n<blockquote><em>If you are methodical and pragmatic like me, the accuracy and reliability of AI responses are paramount. Many commercial AI solutions can and do produce hallucinations-answers that seem plausible but are incorrect. Additionally, the concerns about data privacy and security when using cloud-based solutions might make you want to avoid using personal context. To address these issues, I developed the Retrieval Augmented Generator (RAG) application designed to run locally, ensuring that all data remains secure and responses are both accurate and traceable.</em></blockquote>\n<h3>Why Run Local and What Does ItÂ Take?</h3>\n<p>Running AI locally has significant advantages, including enhanced data privacy, control, and speed. Surprisingly, it doesnâ€™t require a high-end setup. I ran this project on a MacBook Air M3, proving that a mid-range laptop is sufficient. This setup ensures that all data stays on your machine, giving you complete control over your information. More importantly, power to tame the AI with context and knowledge from yourÂ domain.</p>\n<h3>Understanding RAG in SimpleÂ Terms</h3>\n<p>So, what is RAG? Imagine you ask a question and receive an answer thatâ€™s not just generated by an AI but backed by verified documents. RAG works in twoÂ steps:</p>\n<p><strong>1. Retrieving Relevant Documents: </strong>It searches a database of reliable documents to find information related to yourÂ query.</p>\n<p><strong>2. Generating Responses: </strong>Using AI, it crafts a response based on the retrieved information, ensuring the answer is accurate and contextually relevant.</p>\n<p>This approach offers significant benefits over commercial AI apps, which may not always provide traceable answers or may store your data on externalÂ servers.</p>\n<p>That is just it in a nutshellâ€Šâ€”â€Šbut the extent of RAG is tremendous. Chaining and combining responses with your tools of choice, executing entire workflows even.</p>\n<h3>Key Features of the RAG Engine( I named itÂ Llamapp)</h3>\n<p>Hereâ€™s a quick overview of the standout features of the RAGÂ Engine:</p>\n<p><strong>- RewritingÂ </strong>: Refines user queries to avoid distractions and enhance accuracy.</p>\n<p><strong>- Fusion Reciprocal RankingÂ </strong>: Combines results from multiple queries to improve response relevance. Think of google page ranking but tuned forÂ you.</p>\n<p><strong>- Custom Loaders and RetrieversÂ </strong>: Ensures the system can handle various document types and retrieve the most pertinent information.</p>\n<p>I am still exploring the need and various ways we can use this. I want to share this project with the community and invite you to contribute, explore, and collaborate. Whether youâ€™re an AI enthusiast, a developer, or someone curious about the potential of local AI, your input and contributions are invaluable.</p>\n<p><strong><em>Check out the project on GitHub: </em></strong><a href=\"https://github.com/rajatasusual/llamapp?trk=article-ssr-frontend-pulse_little-text-block\"><strong><em>[GitHubÂ link]</em></strong></a></p>\n<p>Letâ€™s connect, share insights, and build something amazing together!</p>\n<p>This project is entirely open source, underscoring my commitment to transparency, collaboration, and shared learning. Dive into the code, tweak it, improve it, and letâ€™s push the boundaries of whatâ€™s possible withÂ AI!</p>\n<p>#AI #MachineLearning #Langchain #Redis #Ollama #InformationRetrieval #LearningJourney #OpenSource #GitHubProject #TechInnovation #LinkedInArticle</p>\n<p><em>Originally published at </em><a href=\"https://www.linkedin.com/pulse/tame-artificial-intelligence-from-your-laptop-rajat-kumar-pfnae/?trackingId=1JjRp6xcT7ObIWtT6qQ8Hw%3D%3D\"><em>https://www.linkedin.com</em></a><em>.</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d91d3d38b9b4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4\">Tame Artificial Intelligence, from your laptop</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*PbGZF4nbTbblAmAijk7Kdw.png\"><figcaption>llamapp beta. (<a href=\"http://github.com/rajatasusual/llamapp\">http://github.com/rajatasusual/llamapp</a>)</figcaption></figure><blockquote>This is a personal project (<a href=\"http://github.com/rajatasusual/llamapp\">http://github.com/rajatasusual/llamapp</a> ) where I explore the extents of layer 2 solutions on top of AI models all run locallyâ€Šâ€”â€Šcontextually aware, no hallucinations and secure.(Shoutout to <strong>Langchain</strong>(<a href=\"https://github.com/langchain-ai\">https://github.com/langchain-ai</a>), a really cool company enabling the â€œpowered by AIâ€ era. P.S. the actual powered by AI and not just a prompt hiding behind a beautiful UI)</blockquote>\n<p>You know the feeling when you get a new laptop and you immediately want to see what breaks it? No? Maybe itâ€™sÂ me.</p>\n<p>Well turns out, running a super powerful (yet tiny by AI standards) <a href=\"https://ai.google.dev/gemma?trk=article-ssr-frontend-pulse_little-text-block\">Google Gemma</a>Â , a vector DB (<a href=\"http://redis.io/\">http://redis.io</a>) and a local server doesnâ€™t. And that it gives far better results than expected.</p>\n<h3>The ElevatorÂ Pitch</h3>\n<blockquote><em>If you are methodical and pragmatic like me, the accuracy and reliability of AI responses are paramount. Many commercial AI solutions can and do produce hallucinations-answers that seem plausible but are incorrect. Additionally, the concerns about data privacy and security when using cloud-based solutions might make you want to avoid using personal context. To address these issues, I developed the Retrieval Augmented Generator (RAG) application designed to run locally, ensuring that all data remains secure and responses are both accurate and traceable.</em></blockquote>\n<h3>Why Run Local and What Does ItÂ Take?</h3>\n<p>Running AI locally has significant advantages, including enhanced data privacy, control, and speed. Surprisingly, it doesnâ€™t require a high-end setup. I ran this project on a MacBook Air M3, proving that a mid-range laptop is sufficient. This setup ensures that all data stays on your machine, giving you complete control over your information. More importantly, power to tame the AI with context and knowledge from yourÂ domain.</p>\n<h3>Understanding RAG in SimpleÂ Terms</h3>\n<p>So, what is RAG? Imagine you ask a question and receive an answer thatâ€™s not just generated by an AI but backed by verified documents. RAG works in twoÂ steps:</p>\n<p><strong>1. Retrieving Relevant Documents: </strong>It searches a database of reliable documents to find information related to yourÂ query.</p>\n<p><strong>2. Generating Responses: </strong>Using AI, it crafts a response based on the retrieved information, ensuring the answer is accurate and contextually relevant.</p>\n<p>This approach offers significant benefits over commercial AI apps, which may not always provide traceable answers or may store your data on externalÂ servers.</p>\n<p>That is just it in a nutshellâ€Šâ€”â€Šbut the extent of RAG is tremendous. Chaining and combining responses with your tools of choice, executing entire workflows even.</p>\n<h3>Key Features of the RAG Engine( I named itÂ Llamapp)</h3>\n<p>Hereâ€™s a quick overview of the standout features of the RAGÂ Engine:</p>\n<p><strong>- RewritingÂ </strong>: Refines user queries to avoid distractions and enhance accuracy.</p>\n<p><strong>- Fusion Reciprocal RankingÂ </strong>: Combines results from multiple queries to improve response relevance. Think of google page ranking but tuned forÂ you.</p>\n<p><strong>- Custom Loaders and RetrieversÂ </strong>: Ensures the system can handle various document types and retrieve the most pertinent information.</p>\n<p>I am still exploring the need and various ways we can use this. I want to share this project with the community and invite you to contribute, explore, and collaborate. Whether youâ€™re an AI enthusiast, a developer, or someone curious about the potential of local AI, your input and contributions are invaluable.</p>\n<p><strong><em>Check out the project on GitHub: </em></strong><a href=\"https://github.com/rajatasusual/llamapp?trk=article-ssr-frontend-pulse_little-text-block\"><strong><em>[GitHubÂ link]</em></strong></a></p>\n<p>Letâ€™s connect, share insights, and build something amazing together!</p>\n<p>This project is entirely open source, underscoring my commitment to transparency, collaboration, and shared learning. Dive into the code, tweak it, improve it, and letâ€™s push the boundaries of whatâ€™s possible withÂ AI!</p>\n<p>#AI #MachineLearning #Langchain #Redis #Ollama #InformationRetrieval #LearningJourney #OpenSource #GitHubProject #TechInnovation #LinkedInArticle</p>\n<p><em>Originally published at </em><a href=\"https://www.linkedin.com/pulse/tame-artificial-intelligence-from-your-laptop-rajat-kumar-pfnae/?trackingId=1JjRp6xcT7ObIWtT6qQ8Hw%3D%3D\"><em>https://www.linkedin.com</em></a><em>.</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d91d3d38b9b4\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/tame-artificial-intelligence-from-your-laptop-d91d3d38b9b4\">Tame Artificial Intelligence, from your laptop</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "enclosure": {},
            "categories": [
                "programming",
                "llm",
                "gpt",
                "ai",
                "technology"
            ]
        },
        {
            "title": "LLMs Donâ€™t Always Hold The Golden Ticket",
            "pubDate": "2024-07-18 17:30:13",
            "link": "https://medium.com/taming-the-llama/llms-dont-always-hold-the-golden-ticket-95b4dca23ad0?source=rss-07ca2cd9b1d2------2",
            "guid": "https://medium.com/p/95b4dca23ad0",
            "author": "Rajat",
            "thumbnail": "",
            "description": "\n<figure><img alt=\"Llamapp logo. A cute llama with a blue circle background\" src=\"https://cdn-images-1.medium.com/max/250/1*hj4UEaB4sxHbLIolQcM0-A.png\"><figcaption>llamapp</figcaption></figure><blockquote>ğŸ‘‹ğŸ¼ Dear tech leaders, do you feel LLMs are the silver bullet for yourÂ teams?</blockquote>\n<h3>Q1. How important is accuracy for yourÂ team?</h3>\n<p><a href=\"https://arxiv.org/abs/2310.01469\">LLMs hallucinate</a>. Would you want to add another check in your teamâ€™s workflows to verify AI responses? Think of LLMs like GPT4 as off the rack suits. They are trained on general data. They try to stitch together the answer with highest statistical match, without traceability or verifiability to back it up. That is why it is called â€œgenerativeâ€ AI in simplerÂ terms.</p>\n<blockquote>Retraining LLMs to increase accuracy is time and cost intensive process, often still not guaranteeing success.</blockquote>\n<h3>Q2. How sensitive &amp; proprietary is the data you workÂ with?</h3>\n<p>Hereâ€™s a thing about LLMsâ€Šâ€”â€Šthese smartypants need a ton of computation power, which means theyâ€™ve got to use a subprocessor like AWS to help them do their thing. Now, If youâ€™d want to leverage niche Domain specific Knowledge, how would you structurally relay it to LLMs? Assuming you manage that, youâ€™d still need to make peace with the fact that your data doesnâ€™t sit on your machinesâ€Šâ€”â€Šitâ€™s quite secure until itâ€™sÂ not.</p>\n<blockquote>If you want to integrate sensitive information from local databases (documents, chats etc) into your workflows, think twice before using publicÂ LLMs.</blockquote>\n<h3>Q3. How â€œgenericâ€ is your ambition?</h3>\n<blockquote>â€œGenius is in the processâ€â€Šâ€”â€ŠSomeoneÂ smart.</blockquote>\n<p>If your work is methodical and pragmatic and youâ€™d truly want to leverage the benefits of automation that AI can provide in your workflows, you would be handicapped by LLMs rigid architecture. With limited contextual windows and still primitive functional execution most commercial LLMs provide, youâ€™d end up using it only for â€œgenericâ€ retrieval and creationâ€Šâ€”â€ŠNot truly achieving a maturity in your processes.</p>\n<blockquote>If your intention is to create a systemic change in a process intensive organisation, LLMs might prove limiting.</blockquote>\n<h3>Final Thoughts</h3>\n<p>LLMs are heavily trained and seemingly can achieve impossible feats. Thatâ€™s only because of the sheer amount of â€œgeneralâ€ data they are trained on. In the end, itâ€™s just math governed by statistical likelihood. So, if your ambition is to integrate your workflows that operate on sensitive or Domain Specific Knowledge (DOKE), youâ€™d want to find an alternative which is not this prone to hallucinations and at the mercy of third-party security.</p>\n<blockquote>There are alternatives. You can still be â€œAI poweredâ€ without slapping prompts toÂ GPT4.</blockquote>\n<h3>Whatâ€™s next?</h3>\n<p>The alternatives. Just like LLMs, there are MLMs and SLMsÂ which:</p>\n<ol>\n<li>\n<strong>Do not require huge computational power</strong>, thereby can be run on local systems. Look at llama3 or gemma byÂ Google.</li>\n<li>\n<strong>Provide flexibility </strong>to connect into your workflows to more from generative to actionable AI.</li>\n<li>\n<strong>Do not hallucinate</strong>, and can sift through your sensitive databases and catalog of information.</li>\n</ol>\n<blockquote>llamapp (github.com/rajatasusual/llamapp) is an open source project designed to run locally, ensuring that all data remains secure and responses are both accurate and traceable.</blockquote>\n<p>#AI #MachineLearning #LLM #SLM #OpenAI #Technology #DigitalTransformation</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=95b4dca23ad0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/llms-dont-always-hold-the-golden-ticket-95b4dca23ad0\">LLMs Donâ€™t Always Hold The Golden Ticket</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "content": "\n<figure><img alt=\"Llamapp logo. A cute llama with a blue circle background\" src=\"https://cdn-images-1.medium.com/max/250/1*hj4UEaB4sxHbLIolQcM0-A.png\"><figcaption>llamapp</figcaption></figure><blockquote>ğŸ‘‹ğŸ¼ Dear tech leaders, do you feel LLMs are the silver bullet for yourÂ teams?</blockquote>\n<h3>Q1. How important is accuracy for yourÂ team?</h3>\n<p><a href=\"https://arxiv.org/abs/2310.01469\">LLMs hallucinate</a>. Would you want to add another check in your teamâ€™s workflows to verify AI responses? Think of LLMs like GPT4 as off the rack suits. They are trained on general data. They try to stitch together the answer with highest statistical match, without traceability or verifiability to back it up. That is why it is called â€œgenerativeâ€ AI in simplerÂ terms.</p>\n<blockquote>Retraining LLMs to increase accuracy is time and cost intensive process, often still not guaranteeing success.</blockquote>\n<h3>Q2. How sensitive &amp; proprietary is the data you workÂ with?</h3>\n<p>Hereâ€™s a thing about LLMsâ€Šâ€”â€Šthese smartypants need a ton of computation power, which means theyâ€™ve got to use a subprocessor like AWS to help them do their thing. Now, If youâ€™d want to leverage niche Domain specific Knowledge, how would you structurally relay it to LLMs? Assuming you manage that, youâ€™d still need to make peace with the fact that your data doesnâ€™t sit on your machinesâ€Šâ€”â€Šitâ€™s quite secure until itâ€™sÂ not.</p>\n<blockquote>If you want to integrate sensitive information from local databases (documents, chats etc) into your workflows, think twice before using publicÂ LLMs.</blockquote>\n<h3>Q3. How â€œgenericâ€ is your ambition?</h3>\n<blockquote>â€œGenius is in the processâ€â€Šâ€”â€ŠSomeoneÂ smart.</blockquote>\n<p>If your work is methodical and pragmatic and youâ€™d truly want to leverage the benefits of automation that AI can provide in your workflows, you would be handicapped by LLMs rigid architecture. With limited contextual windows and still primitive functional execution most commercial LLMs provide, youâ€™d end up using it only for â€œgenericâ€ retrieval and creationâ€Šâ€”â€ŠNot truly achieving a maturity in your processes.</p>\n<blockquote>If your intention is to create a systemic change in a process intensive organisation, LLMs might prove limiting.</blockquote>\n<h3>Final Thoughts</h3>\n<p>LLMs are heavily trained and seemingly can achieve impossible feats. Thatâ€™s only because of the sheer amount of â€œgeneralâ€ data they are trained on. In the end, itâ€™s just math governed by statistical likelihood. So, if your ambition is to integrate your workflows that operate on sensitive or Domain Specific Knowledge (DOKE), youâ€™d want to find an alternative which is not this prone to hallucinations and at the mercy of third-party security.</p>\n<blockquote>There are alternatives. You can still be â€œAI poweredâ€ without slapping prompts toÂ GPT4.</blockquote>\n<h3>Whatâ€™s next?</h3>\n<p>The alternatives. Just like LLMs, there are MLMs and SLMsÂ which:</p>\n<ol>\n<li>\n<strong>Do not require huge computational power</strong>, thereby can be run on local systems. Look at llama3 or gemma byÂ Google.</li>\n<li>\n<strong>Provide flexibility </strong>to connect into your workflows to more from generative to actionable AI.</li>\n<li>\n<strong>Do not hallucinate</strong>, and can sift through your sensitive databases and catalog of information.</li>\n</ol>\n<blockquote>llamapp (github.com/rajatasusual/llamapp) is an open source project designed to run locally, ensuring that all data remains secure and responses are both accurate and traceable.</blockquote>\n<p>#AI #MachineLearning #LLM #SLM #OpenAI #Technology #DigitalTransformation</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=95b4dca23ad0\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/taming-the-llama/llms-dont-always-hold-the-golden-ticket-95b4dca23ad0\">LLMs Donâ€™t Always Hold The Golden Ticket</a> was originally published in <a href=\"https://medium.com/taming-the-llama\">Taming the LLaMa</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n",
            "enclosure": {},
            "categories": [
                "llm",
                "digital-transformation",
                "ai",
                "technology",
                "gpt"
            ]
        }
    ]
}